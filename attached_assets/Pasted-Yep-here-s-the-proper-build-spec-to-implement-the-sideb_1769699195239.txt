Yep — here’s the **proper “build spec”** to implement the sidebar upgrades as real product steps (DB + API + UI + agent runtime). This is written so you can hand it directly to your backend/web/mobile agents.

---

## 0) Target Outcome

You will have an **AI Team sidebar** that supports:

1. **Agent roster + live status**
2. **Activity timeline (what agent is doing)**
3. **Plan → Implement → Test pipeline**
4. **Proposed code changes as a “ChangeSet”**
5. **Approve / Revise / Reject** (human-in-the-loop)
6. **Backend “Agent Profiles”** (model, prompts, context limits, tools)
7. **Context usage meter** (simple + expandable breakdown)

---

## 1) Data Model (DB) — Minimal but Complete

### Tables

#### `agent_profiles`

Stores how each agent behaves.

* `id` (pk) e.g. `backend`, `mobile`, `web`, `ml`
* `display_name` e.g. “Backend Agent”
* `model` e.g. `gpt-4.1`
* `max_context_tokens` int (e.g. 128000)
* `system_prompt` text
* `default_temperature` float
* `tools_enabled` jsonb array (e.g. `["files","db","tests"]`)
* `risk_tolerance` enum (`conservative|balanced|aggressive`)
* `verbosity` enum (`low|normal|high`)
* `enabled` boolean
* timestamps

#### `ai_threads`

One sidebar chat/thread (what you’re showing now).

* `id` (pk)
* `title`
* `created_by_user_id`
* timestamps

#### `ai_messages`

Chat messages in that thread.

* `id` (pk)
* `thread_id` (fk)
* `role` enum (`user|agent|system`)
* `agent_id` nullable (fk agent_profiles)
* `content` text (or json if you want rich blocks)
* `meta` jsonb (store model, token usage, tool calls, etc.)
* timestamps

#### `ai_runs`

A “run” is a single execution for Plan/Implement/Test.

* `id` (pk)
* `thread_id` (fk)
* `mode` enum (`plan|implement|test`)
* `status` enum (`queued|running|needs_approval|completed|failed|cancelled`)
* `requested_by_user_id`
* `goal` text
* `constraints` jsonb
* `agents` jsonb array (e.g. `["backend","mobile"]`)
* `context_snapshot` jsonb (optional: list of file refs / prompt packs)
* timestamps

#### `ai_run_events`

Powers the **live activity timeline**.

* `id` (pk)
* `run_id` (fk)
* `agent_id` (fk)
* `type` enum:

  * `STATUS` (idle/running/waiting)
  * `READ_FILE`
  * `WRITE_FILE`
  * `TOOL_CALL`
  * `PROPOSE_CHANGESET`
  * `TEST_START`
  * `TEST_RESULT`
  * `NEEDS_APPROVAL`
  * `ERROR`
* `message` text (short human-readable)
* `data` jsonb (filename, tool payload, etc.)
* `created_at`

#### `changesets`

A bundle of proposed changes awaiting approval.

* `id` (pk)
* `run_id` (fk)
* `status` enum (`draft|needs_approval|approved|rejected|applied`)
* `summary` text (1–3 lines)
* `risk_level` enum (`low|medium|high`)
* timestamps

#### `changeset_files`

Each file diff in the changeset.

* `id` (pk)
* `changeset_id` (fk)
* `path` text
* `diff_unified` text (unified diff)
* `rationale` text (1–2 lines)
* `risk_notes` text
* `stats` jsonb (`{"added":12,"removed":3}`)
* timestamps

#### `changeset_reviews`

Inline comments / revision requests.

* `id` (pk)
* `changeset_id` (fk)
* `file_id` (fk changeset_files)
* `line_number` int nullable
* `comment` text
* `action` enum (`revise|question|nit|block`)
* `created_by_user_id`
* timestamps

That’s the minimum to support everything cleanly.

---

## 2) API Endpoints (Backend)

### Agent Profile Management

* `GET /v1/ai/agent-profiles`
* `GET /v1/ai/agent-profiles/:id`
* `PUT /v1/ai/agent-profiles/:id` (admin)

### Thread + Messages

* `POST /v1/ai/threads` → create sidebar thread
* `GET /v1/ai/threads/:id` → thread metadata
* `GET /v1/ai/threads/:id/messages?cursor=...`
* `POST /v1/ai/threads/:id/messages` → user message

### Runs (Plan/Implement/Test)

* `POST /v1/ai/runs`

  * body:

    ```json
    {
      "thread_id": "t_123",
      "mode": "implement",
      "goal": "Fix entitlement flag drift",
      "constraints": {"no_mobile_ui_changes": true},
      "agents": ["backend","mobile"],
      "approval_required": true
    }
    ```
* `GET /v1/ai/runs/:id`
* `POST /v1/ai/runs/:id/cancel`
* `GET /v1/ai/runs/:id/events?cursor=...`  (or stream via SSE)

### ChangeSet Flow

* `GET /v1/ai/runs/:id/changesets`
* `GET /v1/ai/changesets/:id`
* `POST /v1/ai/changesets/:id/approve`
* `POST /v1/ai/changesets/:id/reject`
* `POST /v1/ai/changesets/:id/revise`

  * body includes comments / requested edits
* `POST /v1/ai/changesets/:id/apply` (only allowed after approve)

### Optional: Context Packs (recommended)

* `POST /v1/ai/context-packs`
* `GET /v1/ai/context-packs/:id`
* `PUT /v1/ai/context-packs/:id`

A “context pack” is your reusable bundle: project rules, codebase map, architecture decisions, naming conventions, etc.

---

## 3) Realtime: SSE (simplest) for Live Activity

Use Server-Sent Events for:

* run status changes
* events timeline
* progress updates
* “needs approval” transition

**Endpoint**

* `GET /v1/ai/runs/:id/stream` (SSE)

**Event payload**

```json
{
  "type": "READ_FILE",
  "agent_id": "backend",
  "message": "Reading backend/src/entitlements.ts",
  "data": {"path":"backend/src/entitlements.ts"},
  "ts": "2026-01-29T06:12:01Z"
}
```

Frontend just appends to the timeline.

---

## 4) Agent Runtime Contract (How agents report “what they’re doing”)

Your agent orchestrator must emit events at these points:

### Standard lifecycle

1. Run created → `STATUS queued`
2. Agent started → `STATUS running`
3. Any file read/write → `READ_FILE` / `WRITE_FILE`
4. Tool call → `TOOL_CALL`
5. Proposed diff → `PROPOSE_CHANGESET`
6. If approval required → `NEEDS_APPROVAL` + set run status `needs_approval`
7. On approval → apply changes → `STATUS running` then `completed`

### Context Usage Meter

Each agent response should include:

```json
"token_usage": {
  "prompt_tokens": 21000,
  "completion_tokens": 1200,
  "max_context_tokens": 128000,
  "breakdown": {
    "system_prompt": 1500,
    "thread_history": 8000,
    "files": 11000,
    "tools": 500
  }
}
```

This powers the thin bar + “details” view.

---

## 5) UI Spec (Sidebar Components)

### A) Header: “AI Team”

Add:

* small “connected” dot
* run mode pill (Plan/Implement/Test)
* fast/accurate toggle (per run)

### B) Agent Roster Panel (collapsible)

Each row:

* status dot: idle/working/waiting/error
* agent name
* short current action (1 line)
* context bar (optional)

Example:

* `● Backend Agent  — Editing entitlement resolver`
* `◐ Mobile Agent   — Waiting for approval`

### C) Activity Timeline (main upgrade)

A vertical feed, grouped by agent, newest last.
Each event is 1 line; clicking expands data.

### D) Chat messages (your existing area)

When agent speaks, message header contains:

* agent name
* model indicator (collapsed)
* token usage indicator (tiny)

### E) ChangeSet Drawer (right panel or bottom sheet)

Shows:

* Summary, risk
* File list with stats
* Diff viewer
* Rationale per file
* Review comments

Buttons:

* ✅ Approve
* ✏️ Request revisions
* ❌ Reject

### F) Approval UX rules (important)

* “Implement” runs should **stop at needs_approval**
* “Test” runs can be auto-approved to run tests, but **never apply**
* Only “Apply” happens after explicit approval

---

## 6) Prompting System (Conversational → Structured)

When user types in the box, keep it conversational.

When they hit **Plan / Implement / Test**, create a **structured prompt** behind the scenes:

```yaml
goal: "<user goal>"
mode: implement
agents:
  - backend
constraints:
  - no_mobile_ui_changes
approval_required: true
output_contract:
  - must_propose_changeset
  - must_include_tests_if_backend_change
  - must_emit_run_events
```

This is what keeps the “CLI simplicity” while preserving chat.

---

## 7) Backend “Settings” for Context Size + Prompts (what you asked)

Yes: store in `agent_profiles` and optionally a global table:

#### `ai_settings`

* `default_model`
* `default_max_context_tokens`
* `default_approval_required`
* `context_pack_ids` jsonb (global packs always included)
* `safety_rules` text
* `repo_mounts` jsonb (paths allowed)

Then each run resolves its final config as:
**global defaults → agent profile → run overrides**

---

## 8) Implementation Order (so it ships fast)

### Phase 1 (high impact, low complexity)

* DB: `agent_profiles`, `ai_runs`, `ai_run_events`
* API: create run + fetch events
* UI: agent roster + timeline feed
* SSE: live stream

### Phase 2 (human-in-the-loop control)

* DB: `changesets`, `changeset_files`, `changeset_reviews`
* API: propose/approve/revise/reject/apply
* UI: changeset drawer with diff + buttons

### Phase 3 (power features)

* Context packs + context meter breakdown
* Run templates (“Backend Fix”, “Mobile Bug”, “Release Checklist”)

---

## 9) What to tell your agents (copy/paste tasks)

### Backend Agent Tasks

* Add DB schema above
* Add endpoints above
* Add SSE streaming endpoint
* Add “run status machine”
* Ensure implement runs halt at `needs_approval`

### Web/UI Agent Tasks

* Add Agent Roster panel + Timeline
* Add ChangeSet drawer with diff viewer
* Add Approve/Revise/Reject actions
* Add context bar UI (thin + details popover)

### Mobile Agent Tasks (if sidebar is mobile too)

* Mirror roster + timeline
* ChangeSet view can be simplified (list + approve buttons)

---

If you want, I can take your existing current sidebar layout and provide a **pixel-level UI layout** (spacing, typography, subtle glow rules, colors) that matches your “cyberprofessional AI lab / Bloomberg terminal” vibe—without overstimulation.
