# SimpleAide Docker Environment Configuration
# Copy this file to .env.docker and customize as needed

# ===========================================
# Application Settings
# ===========================================
PORT=8521
HOST=0.0.0.0
NODE_ENV=production

# Session secret (auto-generated if not set, but recommended for clusters)
# Example: openssl rand -hex 32
SESSION_SECRET=

# ===========================================
# LLM Backend Selection
# ===========================================
# Choose backend: ollama | vllm
LLM_BACKEND=ollama

# Unified LLM settings (app reads these)
# These are auto-resolved based on LLM_BACKEND if not set
LLM_BASE_URL=
LLM_MODEL=

# ===========================================
# Ollama Configuration
# ===========================================
# Model to use with Ollama (HuggingFace-style tags)
OLLAMA_MODEL=qwen2.5:7b

# Request timeout in milliseconds (default: 120000 = 2 minutes)
OLLAMA_REQUEST_TIMEOUT_MS=120000

# ===========================================
# vLLM / OpenAI-Compatible Configuration
# ===========================================
# HuggingFace model ID for vLLM
VLLM_MODEL=Qwen/Qwen2.5-7B-Instruct

# vLLM inference settings
VLLM_DTYPE=float16
VLLM_MAX_MODEL_LEN=8192

# Optional: API key for remote OpenAI-compatible servers
# (vLLM local doesn't require this)
OPENAI_API_KEY=
OPENAI_ORG_ID=
OPENAI_PROJECT_ID=

# ===========================================
# Optional: External API Keys
# ===========================================
# Uncomment and fill in if using external services

# KAGGLE_USERNAME=your_kaggle_username
# KAGGLE_KEY=your_kaggle_api_key

# HUGGINGFACE_API_KEY=your_hf_api_key

# NGC_API_KEY=your_nvidia_ngc_api_key

# ===========================================
# Optional: GPU Configuration
# ===========================================
# Uncomment to pin to a specific GPU (0, 1, etc.)
# CUDA_VISIBLE_DEVICES=0
